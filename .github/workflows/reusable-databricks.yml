name: reusable-databricks

on:
  workflow_call:
    inputs:
      mode:
        description: "Which lifecycle to run: ci-dev | ci-test | cd-prod"
        required: true
        type: string
      bundle_path:
        description: "Path to the Databricks Asset Bundle (contains databricks.yml)"
        required: true
        type: string
      target:
        description: "Databricks bundle target name (e.g., dev, test, prod)"
        required: true
        type: string
      environment_name:
        description: "GitHub Environment name to apply for secrets/approvals (e.g., DataBricks-Test)"
        required: true
        type: string
      artifact_name:
        description: "Artifact name for build outputs (if applicable)"
        required: false
        default: "dw_bundle_artifact"
        type: string

      run_asset_validation:
        description: "Run metadata-driven asset validation job after deploy (ci-test recommended)"
        required: false
        default: true
        type: boolean
      asset_validation_notebook_path:
        description: "Databricks notebook path for asset validation driver"
        required: false
        default: "/Repos/ci/asset_validation_driver"
        type: string
      asset_validation_cluster_id_secret:
        description: "Name of the GitHub secret containing the cluster id used for validations"
        required: false
        default: "DATABRICKS_VALIDATION_CLUSTER_ID"
        type: string

      promote_artifact_only:
        description: "If true, skip build and only deploy a previously-built artifact (cd-prod use-case)"
        required: false
        default: false
        type: boolean

      run_post_deployment_tests:
        description: "Run post-deploy tests (summary + optional smoke)."
        required: false
        default: false
        type: boolean
      post_deploy_summary_output:
        description: "Output format for databricks bundle summary: text | json | both"
        required: false
        default: "text"
        type: string

      # Job/Notebook smoke (compute plane)
      run_post_deploy_smoke:
        description: "If true, run a job/notebook smoke test after deploy (compute plane)."
        required: false
        default: false
        type: boolean
      smoke_mode:
        description: "How to run job/notebook smoke tests: notebook | bundle-run"
        required: false
        default: "notebook"
        type: string
      smoke_notebook_path:
        description: "Notebook path used for smoke (when smoke_mode=notebook)"
        required: false
        default: ""
        type: string
      smoke_job_key:
        description: "Job key to run via databricks bundle run (when smoke_mode=bundle-run)"
        required: false
        default: ""
        type: string

      # SQL Warehouse smoke (serving plane)
      run_sql_smoke:
        description: "If true, run SQL smoke tests against a Databricks SQL Warehouse after deploy."
        required: false
        default: false
        type: boolean
      sql_warehouse_id_secret:
        description: "Name of the GitHub secret containing the SQL Warehouse ID."
        required: false
        default: "DATABRICKS_SQL_WAREHOUSE_ID"
        type: string
      sql_smoke_queries:
        description: |
          "One SQL statement per line to execute as post-deploy smoke (examples: SELECT 1; SHOW SCHEMAS; SELECT count(*) FROM ...)."
        required: false
        default: |
          SELECT 1
        type: string

    secrets:
      DATABRICKS_HOST:
        required: true
      DATABRICKS_TOKEN:
        required: true
      DATABRICKS_VALIDATION_CLUSTER_ID:
        required: true
      DATABRICKS_SQL_WAREHOUSE_ID:
        required: true

jobs:
  databricks:
    name: ${{ inputs.mode }}
    runs-on: ubuntu-latest
    environment: ${{ inputs.environment_name }}
    permissions:
      contents: read
    env:
      DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
      DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
      DATABRICKS_VALIDATION_CLUSTER_ID: ${{ secrets.DATABRICKS_VALIDATION_CLUSTER_ID }}
      BUNDLE_PATH: ${{ inputs.bundle_path }}
      TARGET: ${{ inputs.target }}
      ARTIFACT_NAME: ${{ inputs.artifact_name }}
      MODE: ${{ inputs.mode }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install tooling
        run: |
          python -m pip install --upgrade pip
          # Do NOT install legacy databricks-cli (it overrides the `databricks` command)
          pip install databricks-sdk

          sudo apt-get update
          sudo apt-get install -y jq zip

          # Install the NEW Databricks CLI (supports `databricks bundle ...`)
          curl -fsSL https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh | sh

          echo "Databricks CLI path:"
          which databricks
          echo "Databricks CLI version:"
          databricks version
          echo "Databricks bundle help:"
          databricks bundle -h

      - name: Validate inputs
        run: |
          case "$MODE" in
            ci-dev|ci-test|cd-prod) ;;
            *) echo "Invalid mode: $MODE (expected ci-dev|ci-test|cd-prod)"; exit 2;;
          esac

          if [ ! -f "$BUNDLE_PATH/databricks.yml" ] && [ ! -f "$BUNDLE_PATH/databricks.yaml" ]; then
            echo "Could not find databricks.yml under bundle_path: $BUNDLE_PATH"
            exit 2
          fi

          case "${{ inputs.post_deploy_summary_output }}" in
            text|json|both) ;;
            *) echo "Invalid post_deploy_summary_output: ${{ inputs.post_deploy_summary_output }} (expected text|json|both)"; exit 2;;
          esac

          case "${{ inputs.smoke_mode }}" in
            notebook|bundle-run) ;;
            *) echo "Invalid smoke_mode: ${{ inputs.smoke_mode }} (expected notebook|bundle-run)"; exit 2;;
          esac

      - name: Sanity check Databricks CLI
        run: |
          which databricks
          databricks version
          databricks bundle -h

      - name: Bundle validate
        run: |
          cd "$BUNDLE_PATH"
          databricks bundle validate --target "$TARGET" --var="validation_cluster_id=$DATABRICKS_VALIDATION_CLUSTER_ID"

      # Build/package step (optional)
      - name: Package bundle content (artifact)
        if: ${{ inputs.promote_artifact_only == false }}
        run: |
          mkdir -p dist
          # Package the bundle directory as-is (teams can change this later to a curated bundle package)
          zip -r "dist/${ARTIFACT_NAME}.zip" "$BUNDLE_PATH" -x "*.git*"

      - name: Upload artifact
        if: ${{ inputs.promote_artifact_only == false }}
        uses: actions/upload-artifact@v4
        with:
          name: ${{ inputs.artifact_name }}
          path: dist/${{ inputs.artifact_name }}.zip
          if-no-files-found: error
          retention-days: 14

      - name: Bundle deploy
        run: |
          cd "$BUNDLE_PATH"
          databricks bundle deploy --target "$TARGET" --auto-approve --var="validation_cluster_id=$DATABRICKS_VALIDATION_CLUSTER_ID"

      - name: Post-deploy verification (bundle summary)
        if: ${{ inputs.run_post_deployment_tests }}
        run: |
          cd "$BUNDLE_PATH"

          case "${{ inputs.post_deploy_summary_output }}" in
            json)
              databricks bundle summary --target "$TARGET" --output json > post_deploy_summary.json
              echo "Wrote post_deploy_summary.json"
              ;;
            both)
              databricks bundle summary --target "$TARGET"
              databricks bundle summary --target "$TARGET" --output json > post_deploy_summary.json
              echo "Wrote post_deploy_summary.json"
              ;;
            *)
              databricks bundle summary --target "$TARGET"
              ;;
          esac

      - name: Asset validation (metadata-driven)
        if: ${{ inputs.mode == 'ci-test' && inputs.run_asset_validation }}
        env:
          DATABRICKS_VALIDATION_CLUSTER_ID: ${{ secrets.DATABRICKS_VALIDATION_CLUSTER_ID }}
          NOTEBOOK_PATH: ${{ inputs.asset_validation_notebook_path }}
          ENV_NAME: ${{ inputs.target }}
          COMMIT_SHA: ${{ github.sha }}
        run: |
          if [ -z "$DATABRICKS_VALIDATION_CLUSTER_ID" ]; then
            echo "Missing secret: DATABRICKS_VALIDATION_CLUSTER_ID in environment: ${{ inputs.environment_name }}"
            exit 2
          fi

          python - <<'PY'
          import os, time, sys
          from databricks.sdk import WorkspaceClient
          from databricks.sdk.errors.platform import NotFound
          from databricks.sdk.service import jobs

          cluster_id = os.environ["DATABRICKS_VALIDATION_CLUSTER_ID"]
          notebook_path = os.environ["NOTEBOOK_PATH"]
          env_name = os.environ["ENV_NAME"]
          commit_sha = os.environ["COMMIT_SHA"]

          w = WorkspaceClient()

          # Fail fast with a clearer error if the cluster id is wrong for this workspace.
          try:
              w.clusters.get(cluster_id)
          except NotFound:
              print(f"Cluster '{cluster_id}' does not exist in this workspace. Check GitHub Environment secrets for {env_name}.", file=sys.stderr)
              raise

          run_name = f"asset-validation-{commit_sha}"
          task = jobs.SubmitTask(
              task_key="asset_validation",
              existing_cluster_id=cluster_id,
              notebook_task=jobs.NotebookTask(
                  notebook_path=notebook_path,
                  base_parameters={"env": env_name, "commit_sha": commit_sha},
              ),
          )

          run = w.jobs.submit(run_name=run_name, tasks=[task])
          run_id = run.run_id
          print(f"Submitted Databricks job run_id={run_id}")

          terminal = {"TERMINATED", "INTERNAL_ERROR", "SKIPPED"}
          while True:
              r = w.jobs.get_run(run_id)
              state = r.state.life_cycle_state.name if r.state and r.state.life_cycle_state else None
              result = r.state.result_state.name if r.state and r.state.result_state else None
              print(f"life_cycle_state={state} result_state={result}")
              if state in terminal:
                  break
              time.sleep(10)

          final = w.jobs.get_run(run_id)
          final_result = final.state.result_state.name if final.state and final.state.result_state else None
          if final_result != "SUCCESS":
              print(f"Asset validation failed (result_state={final_result}). run_id={run_id}", file=sys.stderr)
              sys.exit(1)

          print("Asset validation SUCCESS")
          PY

      - name: Post-deploy smoke (job/notebook, compute plane)
        if: ${{ inputs.run_post_deployment_tests && inputs.run_post_deploy_smoke }}
        env:
          DATABRICKS_VALIDATION_CLUSTER_ID: ${{ secrets.DATABRICKS_VALIDATION_CLUSTER_ID }}
          SMOKE_MODE: ${{ inputs.smoke_mode }}
          SMOKE_NOTEBOOK_PATH: ${{ inputs.smoke_notebook_path }}
          SMOKE_JOB_KEY: ${{ inputs.smoke_job_key }}
          ENV_NAME: ${{ inputs.target }}
          COMMIT_SHA: ${{ github.sha }}
        run: |
          if [ -z "$SMOKE_MODE" ]; then
            echo "smoke_mode is empty."
            exit 2
          fi

          if [ "$SMOKE_MODE" = "bundle-run" ]; then
            if [ -z "$SMOKE_JOB_KEY" ]; then
              echo "smoke_job_key is empty (required when smoke_mode=bundle-run)."
              exit 2
            fi
            cd "$BUNDLE_PATH"
            databricks bundle run "$SMOKE_JOB_KEY" --target "$TARGET" --var="validation_cluster_id=$DATABRICKS_VALIDATION_CLUSTER_ID"
            exit 0
          fi

          # notebook mode
          if [ -z "$SMOKE_NOTEBOOK_PATH" ]; then
            echo "smoke_notebook_path is empty (required when smoke_mode=notebook)."
            exit 2
          fi
          if [ -z "$DATABRICKS_VALIDATION_CLUSTER_ID" ]; then
            echo "Missing secret: DATABRICKS_VALIDATION_CLUSTER_ID (required for smoke notebook on an existing cluster)."
            exit 2
          fi

          python - <<'PY'
          import os, time, sys
          from databricks.sdk import WorkspaceClient
          from databricks.sdk.errors.platform import NotFound
          from databricks.sdk.service import jobs

          cluster_id = os.environ["DATABRICKS_VALIDATION_CLUSTER_ID"]
          notebook_path = os.environ["SMOKE_NOTEBOOK_PATH"]
          env_name = os.environ["ENV_NAME"]
          commit_sha = os.environ["COMMIT_SHA"]

          w = WorkspaceClient()

          # Fail fast with a clearer error if the cluster id is wrong for this workspace.
          try:
              w.clusters.get(cluster_id)
          except NotFound:
              print(f"Cluster '{cluster_id}' does not exist in this workspace. Check GitHub Environment secrets for {env_name}.", file=sys.stderr)
              raise

          run_name = f"post-deploy-smoke-{commit_sha}"
          task = jobs.SubmitTask(
              task_key="post_deploy_smoke",
              existing_cluster_id=cluster_id,
              notebook_task=jobs.NotebookTask(
                  notebook_path=notebook_path,
                  base_parameters={"env": env_name, "commit_sha": commit_sha},
              ),
          )

          run = w.jobs.submit(run_name=run_name, tasks=[task])
          run_id = run.run_id
          print(f"Submitted smoke run_id={run_id}")

          terminal = {"TERMINATED", "INTERNAL_ERROR", "SKIPPED"}
          while True:
              r = w.jobs.get_run(run_id)
              state = r.state.life_cycle_state.name if r.state and r.state.life_cycle_state else None
              result = r.state.result_state.name if r.state and r.state.result_state else None
              print(f"life_cycle_state={state} result_state={result}")
              if state in terminal:
                  break
              time.sleep(10)

          final = w.jobs.get_run(run_id)
          final_result = final.state.result_state.name if final.state and final.state.result_state else None
          if final_result != "SUCCESS":
              print(f"Smoke failed (result_state={final_result}). run_id={run_id}", file=sys.stderr)
              sys.exit(1)

          print("Smoke SUCCESS")
          PY

      - name: Post-deploy smoke (SQL Warehouse, serving plane)
        if: ${{ inputs.run_post_deployment_tests && inputs.run_sql_smoke }}
        env:
          DATABRICKS_SQL_WAREHOUSE_ID: ${{ secrets.DATABRICKS_SQL_WAREHOUSE_ID }}
          SQL_SMOKE_QUERIES: ${{ inputs.sql_smoke_queries }}
          ENV_NAME: ${{ inputs.target }}
        run: |
          if [ -z "$DATABRICKS_SQL_WAREHOUSE_ID" ]; then
            echo "Missing secret: DATABRICKS_SQL_WAREHOUSE_ID in environment: ${{ inputs.environment_name }}"
            exit 2
          fi

          python - <<'PY'
          import os, time, sys
          from databricks.sdk import WorkspaceClient
          from databricks.sdk.errors.platform import NotFound

          warehouse_id = os.environ["DATABRICKS_SQL_WAREHOUSE_ID"]
          env_name = os.environ.get("ENV_NAME", "unknown")

          raw = os.environ.get("SQL_SMOKE_QUERIES", "SELECT 1")
          queries = [q.strip() for q in raw.splitlines() if q.strip()]

          w = WorkspaceClient()

          # Ensure warehouse exists, and try to start it if not running.
          try:
              wh = w.warehouses.get(warehouse_id)
          except NotFound:
              print(f"SQL Warehouse '{warehouse_id}' does not exist in this workspace. Check GitHub Environment secrets for {env_name}.", file=sys.stderr)
              raise

          state = getattr(wh, "state", None)
          state_name = state.name if state else None
          print(f"Warehouse state={state_name}")

          if state_name and state_name != "RUNNING":
              print("Starting warehouse...")
              w.warehouses.start(warehouse_id)

              # Poll for RUNNING (up to ~10 minutes)
              deadline = time.time() + 600
              while time.time() < deadline:
                  wh = w.warehouses.get(warehouse_id)
                  state = getattr(wh, "state", None)
                  state_name = state.name if state else None
                  print(f"Warehouse state={state_name}")
                  if state_name == "RUNNING":
                      break
                  time.sleep(10)

              if state_name != "RUNNING":
                  print(f"Warehouse did not reach RUNNING state (last_state={state_name}).", file=sys.stderr)
                  sys.exit(1)

          def wait_statement(statement_id: str, timeout_s: int = 300):
              deadline = time.time() + timeout_s
              while True:
                  resp = w.statement_execution.get_statement(statement_id)
                  status = resp.status
                  state = status.state.name if status and status.state else None
                  print(f"statement_id={statement_id} state={state}")
                  if state in ("SUCCEEDED", "FAILED", "CANCELED", "CLOSED"):
                      return resp
                  if time.time() > deadline:
                      raise TimeoutError(f"Timed out waiting for statement {statement_id} (last_state={state})")
                  time.sleep(2)

          # Execute each statement and require success.
          for i, q in enumerate(queries, start=1):
              print(f"\n--- SQL smoke [{i}/{len(queries)}] ---\n{q}\n")
              exec_resp = w.statement_execution.execute_statement(
                  warehouse_id=warehouse_id,
                  statement=q,
                  wait_timeout="0s",  # async; we poll
              )
              stmt_id = exec_resp.statement_id
              final = wait_statement(stmt_id, timeout_s=300)
              status = final.status
              state = status.state.name if status and status.state else None
              if state != "SUCCEEDED":
                  msg = status.error.message if status and status.error else "Unknown error"
                  print(f"SQL smoke FAILED: state={state} error={msg}", file=sys.stderr)
                  sys.exit(1)

          print("\nSQL smoke SUCCESS")
          PY
