name: reusable-databricks

on:
  workflow_call:
    inputs:
      mode:
        description: "Which lifecycle to run: ci-dev | ci-test | cd-prod"
        required: true
        type: string
      bundle_path:
        description: "Path to the Databricks Asset Bundle (contains databricks.yml)"
        required: true
        type: string
      target:
        description: "Databricks bundle target name (e.g., dev, test, prod)"
        required: true
        type: string
      environment_name:
        description: "GitHub Environment name to apply for secrets/approvals (e.g., DataBricks-Test)"
        required: true
        type: string
      artifact_name:
        description: "Artifact name for build outputs (if applicable)"
        required: false
        default: "dw_bundle_artifact"
        type: string

      run_asset_validation:
        description: "Run metadata-driven asset validation job after deploy (ci-test recommended)"
        required: false
        default: true
        type: boolean
      asset_validation_notebook_path:
        description: "Databricks notebook path for asset validation driver"
        required: false
        default: "/Repos/ci/asset_validation_driver"
        type: string
      asset_validation_cluster_id_secret:
        description: "Name of the GitHub secret containing the cluster id used for validations"
        required: false
        default: "DATABRICKS_VALIDATION_CLUSTER_ID"
        type: string

      promote_artifact_only:
        description: "If true, skip build and only deploy a previously-built artifact (cd-prod use-case)"
        required: false
        default: false
        type: boolean

      run_post_deployment_tests:
        description: "Run post-deploy tests (summary + optional smoke)."
        required: false
        default: false
        type: boolean
      post_deploy_summary_output:
        description: "Output format for databricks bundle summary: text | json | both"
        required: false
        default: "text"
        type: string

      # Job/Notebook smoke (compute plane)
      run_post_deploy_smoke:
        description: "If true, run a job/notebook smoke test after deploy (compute plane)."
        required: false
        default: false
        type: boolean
      smoke_mode:
        description: "How to run job/notebook smoke tests: notebook | bundle-run"
        required: false
        default: "notebook"
        type: string
      smoke_notebook_path:
        description: "Notebook path used for smoke (when smoke_mode=notebook)"
        required: false
        default: ""
        type: string
      smoke_job_key:
        description: "Job key to run via databricks bundle run (when smoke_mode=bundle-run)"
        required: false
        default: ""
        type: string

      # SQL Warehouse smoke (serving plane)
      run_sql_smoke:
        description: "If true, run SQL smoke tests against a Databricks SQL Warehouse after deploy."
        required: false
        default: false
        type: boolean
      sql_warehouse_id_secret:
        description: "Name of the GitHub secret containing the SQL Warehouse ID."
        required: false
        default: "DATABRICKS_SQL_WAREHOUSE_ID"
        type: string
      sql_smoke_queries:
        description: |
          "One SQL statement per line to execute as post-deploy smoke (examples: SELECT 1; SHOW SCHEMAS; SELECT count(*) FROM ...)."
        required: false
        default: |
          SELECT 1
        type: string

      # Governance maturity (telemetry first, enforcement later)
      run_maturity_collect:
        description: "If true, run maturity metric collection after deploy (safe; never fails)."
        required: false
        default: false
        type: boolean
      run_maturity_ci_check:
        description: "If true, run maturity CI gate check after deploy (may fail depending on enforcement_mode / override)."
        required: false
        default: false
        type: boolean
      run_maturity_scorecard_status_load:
        description: "If true, load scorecard check statuses after deploy (non-blocking)."
        required: false
        default: false
        type: boolean
      run_maturity_scorecard_eval:
        description: "If true, evaluate scorecard results after deploy (non-blocking)."
        required: false
        default: false
        type: boolean
      run_maturity_scorecard_evidence_stub:
        description: "If true, run evidence stub (dry-run by default) after deploy."
        required: false
        default: false
        type: boolean
      maturity_scorecard_status_job_key:
        description: "Bundle job key for scorecard status loader."
        required: false
        default: "maturity_scorecard_status_load"
        type: string
      maturity_scorecard_eval_job_key:
        description: "Bundle job key for scorecard evaluation."
        required: false
        default: "maturity_scorecard_eval"
        type: string
      maturity_scorecard_evidence_stub_job_key:
        description: "Bundle job key for evidence stub."
        required: false
        default: "maturity_scorecard_evidence_stub"
        type: string
      maturity_scorecard_source:
        description: "Scorecard source for evaluation (embedded|workspace_csv)."
        required: false
        default: ""
        type: string
      maturity_scorecard_path:
        description: "Scorecard CSV path used when source=workspace_csv."
        required: false
        default: ""
        type: string
      maturity_scorecard_updated_by:
        description: "Updated_by value for status loader."
        required: false
        default: "bundle"
        type: string
      maturity_scorecard_evidence_stub_write_mode:
        description: "Write mode for evidence stub: dry_run|append."
        required: false
        default: "dry_run"
        type: string
      maturity_scorecard_evidence_stub_status_path:
        description: "Optional status CSV path for evidence stub."
        required: false
        default: ""
        type: string
      maturity_scorecard_evidence_stub_updated_by:
        description: "Updated_by value for evidence stub."
        required: false
        default: "evidence_stub"
        type: string
      maturity_enforcement_mode:
        description: "Override enforcement mode for this run: off|warn|block. Empty means use config table."
        required: false
        default: ""
        type: string
      maturity_override:
        description: "Override behavior for this run: none|warn_only|skip"
        required: false
        default: "none"
        type: string
      bundle_name:
        description: "Bundle name for deployment tracking (governance_maturity.bundle_deployments)"
        required: false
        default: ""
        type: string
      databricks_cli_version:
        description: "Pinned Databricks CLI installer release tag."
        required: false
        default: "v0.277.0"
        type: string

    secrets:
      DATABRICKS_HOST:
        required: true
      DATABRICKS_TOKEN:
        required: true
      DATABRICKS_VALIDATION_CLUSTER_ID:
        required: false
      DATABRICKS_SQL_WAREHOUSE_ID:
        required: false

permissions:
  contents: read

jobs:
  databricks:
    name: ${{ inputs.mode }}
    runs-on: ubuntu-latest
    timeout-minutes: 90
    environment: ${{ inputs.environment_name }}
    permissions:
      contents: read
    env:
      DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
      DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
      BUNDLE_PATH: ${{ inputs.bundle_path }}
      TARGET: ${{ inputs.target }}
      ARTIFACT_NAME: ${{ inputs.artifact_name }}
      MODE: ${{ inputs.mode }}
      ENVIRONMENT_NAME_INPUT: ${{ inputs.environment_name }}
      POST_DEPLOY_SUMMARY_OUTPUT: ${{ inputs.post_deploy_summary_output }}
      SMOKE_MODE_INPUT: ${{ inputs.smoke_mode }}
      MATURITY_OVERRIDE_INPUT: ${{ inputs.maturity_override }}
      MATURITY_ENFORCEMENT_MODE_INPUT: ${{ inputs.maturity_enforcement_mode }}
      MATURITY_SCORECARD_STATUS_JOB_KEY_INPUT: ${{ inputs.maturity_scorecard_status_job_key }}
      MATURITY_SCORECARD_EVAL_JOB_KEY_INPUT: ${{ inputs.maturity_scorecard_eval_job_key }}
      MATURITY_SCORECARD_EVIDENCE_STUB_JOB_KEY_INPUT: ${{ inputs.maturity_scorecard_evidence_stub_job_key }}
      MATURITY_SCORECARD_SOURCE_INPUT: ${{ inputs.maturity_scorecard_source }}
      MATURITY_SCORECARD_PATH_INPUT: ${{ inputs.maturity_scorecard_path }}
      MATURITY_SCORECARD_UPDATED_BY_INPUT: ${{ inputs.maturity_scorecard_updated_by }}
      MATURITY_SCORECARD_EVIDENCE_STUB_WRITE_MODE_INPUT: ${{ inputs.maturity_scorecard_evidence_stub_write_mode }}
      MATURITY_SCORECARD_EVIDENCE_STUB_STATUS_PATH_INPUT: ${{ inputs.maturity_scorecard_evidence_stub_status_path }}
      MATURITY_SCORECARD_EVIDENCE_STUB_UPDATED_BY_INPUT: ${{ inputs.maturity_scorecard_evidence_stub_updated_by }}
      RUN_ASSET_VALIDATION_INPUT: ${{ inputs.run_asset_validation }}
      RUN_POST_DEPLOY_SMOKE_INPUT: ${{ inputs.run_post_deploy_smoke }}
      RUN_SQL_SMOKE_INPUT: ${{ inputs.run_sql_smoke }}
      ASSET_VALIDATION_CLUSTER_ID_SECRET_INPUT: ${{ inputs.asset_validation_cluster_id_secret }}
      SQL_WAREHOUSE_ID_SECRET_INPUT: ${{ inputs.sql_warehouse_id_secret }}
      SMOKE_JOB_KEY_INPUT: ${{ inputs.smoke_job_key }}
      SMOKE_NOTEBOOK_PATH_INPUT: ${{ inputs.smoke_notebook_path }}
      BUNDLE_NAME_INPUT: ${{ inputs.bundle_name }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install tooling
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          # Do NOT install legacy databricks-cli (it overrides the `databricks` command)
          pip install databricks-sdk==0.53.0
          sudo apt-get update
          sudo apt-get install -y jq zip
          curl -fsSL "https://raw.githubusercontent.com/databricks/setup-cli/${{ inputs.databricks_cli_version }}/install.sh" | sh
          databricks version
          databricks bundle -h

      - name: Resolve optional secrets
        shell: bash
        run: |
          set -euo pipefail
          validation_cluster_id="${{ secrets[inputs.asset_validation_cluster_id_secret] }}"
          sql_warehouse_id="${{ secrets[inputs.sql_warehouse_id_secret] }}"
          if [ -n "$validation_cluster_id" ]; then
            echo "::add-mask::$validation_cluster_id"
          fi
          if [ -n "$sql_warehouse_id" ]; then
            echo "::add-mask::$sql_warehouse_id"
          fi
          echo "VALIDATION_CLUSTER_ID=$validation_cluster_id" >> "$GITHUB_ENV"
          echo "SQL_WAREHOUSE_ID=$sql_warehouse_id" >> "$GITHUB_ENV"

      - name: Validate inputs
        run: |
          set -euo pipefail
          case "$MODE" in
            ci-dev|ci-test|cd-prod) ;;
            *) echo "Invalid mode: $MODE (expected ci-dev|ci-test|cd-prod)"; exit 2;;
          esac

          if [[ "$BUNDLE_PATH" == /* ]] || [[ "$BUNDLE_PATH" == *".."* ]]; then
            echo "Invalid bundle_path: $BUNDLE_PATH (must be relative and must not contain '..')"
            exit 2
          fi

          if [ ! -f "$BUNDLE_PATH/databricks.yml" ] && [ ! -f "$BUNDLE_PATH/databricks.yaml" ]; then
            echo "Could not find databricks.yml under bundle_path: $BUNDLE_PATH"
            exit 2
          fi

          case "$POST_DEPLOY_SUMMARY_OUTPUT" in
            text|json|both) ;;
            *) echo "Invalid post_deploy_summary_output: $POST_DEPLOY_SUMMARY_OUTPUT (expected text|json|both)"; exit 2;;
          esac

          case "$SMOKE_MODE_INPUT" in
            notebook|bundle-run) ;;
            *) echo "Invalid smoke_mode: $SMOKE_MODE_INPUT (expected notebook|bundle-run)"; exit 2;;
          esac

          case "$MATURITY_OVERRIDE_INPUT" in
            none|warn_only|skip) ;;
            *) echo "Invalid maturity_override: $MATURITY_OVERRIDE_INPUT (expected none|warn_only|skip)"; exit 2;;
          esac

          if [ -n "$MATURITY_ENFORCEMENT_MODE_INPUT" ]; then
            case "$MATURITY_ENFORCEMENT_MODE_INPUT" in
              off|warn|block) ;;
              *) echo "Invalid maturity_enforcement_mode: $MATURITY_ENFORCEMENT_MODE_INPUT (expected off|warn|block)"; exit 2;;
            esac
          fi

          if [ "$RUN_ASSET_VALIDATION_INPUT" = "true" ] && [ -z "${VALIDATION_CLUSTER_ID:-}" ]; then
            echo "run_asset_validation=true requires a cluster id secret: $ASSET_VALIDATION_CLUSTER_ID_SECRET_INPUT"
            exit 2
          fi

          if [ "$RUN_POST_DEPLOY_SMOKE_INPUT" = "true" ] && [ "$SMOKE_MODE_INPUT" = "notebook" ] && [ -z "${VALIDATION_CLUSTER_ID:-}" ]; then
            echo "smoke_mode=notebook requires a cluster id secret: $ASSET_VALIDATION_CLUSTER_ID_SECRET_INPUT"
            exit 2
          fi

          if [ "$RUN_SQL_SMOKE_INPUT" = "true" ] && [ -z "${SQL_WAREHOUSE_ID:-}" ]; then
            echo "run_sql_smoke=true requires a warehouse id secret: $SQL_WAREHOUSE_ID_SECRET_INPUT"
            exit 2
          fi

          case "$MODE:$TARGET:$ENVIRONMENT_NAME_INPUT" in
            ci-dev:dev:DataBricks-Dev|ci-test:test:DataBricks-Test|cd-prod:prod:DataBricks-Prod) ;;
            *) echo "Invalid mode/target/environment_name combination: $MODE/$TARGET/$ENVIRONMENT_NAME_INPUT"; exit 2;;
          esac

          if [ "$MODE" = "cd-prod" ] && [ "${GITHUB_REF_NAME:-}" != "prod" ]; then
            echo "cd-prod is only allowed when caller ref is prod. Current ref: ${GITHUB_REF_NAME:-unknown}"
            exit 2
          fi

      - name: Bundle validate
        shell: bash
        run: |
          set -euo pipefail
          cd "$BUNDLE_PATH"
          extra_args=()
          if [ -n "${VALIDATION_CLUSTER_ID:-}" ]; then
            extra_args+=(--var "validation_cluster_id=$VALIDATION_CLUSTER_ID")
          fi
          databricks bundle validate --target "$TARGET" "${extra_args[@]}"

      - name: Package bundle content (artifact)
        if: ${{ inputs.promote_artifact_only == false }}
        run: |
          set -euo pipefail
          mkdir -p dist
          zip -r "dist/${ARTIFACT_NAME}.zip" "$BUNDLE_PATH" -x "*.git*"

      - name: Upload artifact
        if: ${{ inputs.promote_artifact_only == false }}
        uses: actions/upload-artifact@v4
        with:
          name: ${{ inputs.artifact_name }}
          path: dist/${{ inputs.artifact_name }}.zip
          if-no-files-found: error
          retention-days: 14

      - name: Bundle deploy
        shell: bash
        run: |
          set -euo pipefail
          cd "$BUNDLE_PATH"
          extra_args=()
          if [ -n "${VALIDATION_CLUSTER_ID:-}" ]; then
            extra_args+=(--var "validation_cluster_id=$VALIDATION_CLUSTER_ID")
          fi
          databricks bundle deploy --target "$TARGET" --auto-approve "${extra_args[@]}"

      - name: Post-deploy bundle summary
        if: ${{ inputs.run_post_deployment_tests }}
        shell: bash
        run: |
          set -euo pipefail
          cd "$BUNDLE_PATH"
          case "${{ inputs.post_deploy_summary_output }}" in
            text)
              databricks bundle summary --target "$TARGET"
              ;;
            json)
              databricks bundle summary --target "$TARGET" --output json | tee bundle-summary.json
              ;;
            both)
              databricks bundle summary --target "$TARGET"
              databricks bundle summary --target "$TARGET" --output json | tee bundle-summary.json
              ;;
          esac

      - name: Post-deploy smoke (bundle run)
        if: ${{ inputs.run_post_deploy_smoke && inputs.smoke_mode == 'bundle-run' }}
        shell: bash
        run: |
          set -euo pipefail
          if [ -z "$SMOKE_JOB_KEY_INPUT" ]; then
            echo "smoke_mode=bundle-run requires smoke_job_key"
            exit 2
          fi
          cd "$BUNDLE_PATH"
          extra_args=()
          if [ -n "${VALIDATION_CLUSTER_ID:-}" ]; then
            extra_args+=(--var "validation_cluster_id=$VALIDATION_CLUSTER_ID")
          fi
          databricks bundle run "$SMOKE_JOB_KEY_INPUT" --target "$TARGET" "${extra_args[@]}"

      - name: Post-deploy smoke (notebook submit)
        if: ${{ inputs.run_post_deploy_smoke && inputs.smoke_mode == 'notebook' }}
        shell: bash
        run: |
          set -euo pipefail
          if [ -z "$SMOKE_NOTEBOOK_PATH_INPUT" ]; then
            echo "smoke_mode=notebook requires smoke_notebook_path"
            exit 2
          fi
          run_output="$(databricks jobs submit --json "$(jq -n \
            --arg cid "$VALIDATION_CLUSTER_ID" \
            --arg path "$SMOKE_NOTEBOOK_PATH_INPUT" \
            '{
              run_name: "github-smoke-${{ inputs.target }}-${{ github.run_id }}",
              tasks: [
                {
                  task_key: "smoke_notebook",
                  existing_cluster_id: $cid,
                  notebook_task: { notebook_path: $path }
                }
              ]
            }'
          )")"
          run_id="$(echo "$run_output" | jq -r '.run_id')"
          if [ -z "$run_id" ] || [ "$run_id" = "null" ]; then
            echo "Failed to create smoke test run."
            echo "$run_output"
            exit 1
          fi
          echo "Submitted smoke run_id=$run_id"
          for attempt in $(seq 1 60); do
            state_json="$(databricks jobs get-run "$run_id")"
            life_cycle_state="$(echo "$state_json" | jq -r '.state.life_cycle_state')"
            result_state="$(echo "$state_json" | jq -r '.state.result_state // empty')"
            echo "Smoke run state (attempt $attempt): $life_cycle_state result: ${result_state:-n/a}"
            if [ "$life_cycle_state" = "TERMINATED" ]; then
              if [ "$result_state" = "SUCCESS" ]; then
                exit 0
              fi
              echo "$state_json"
              exit 1
            fi
            if [ "$life_cycle_state" = "INTERNAL_ERROR" ] || [ "$life_cycle_state" = "SKIPPED" ]; then
              echo "$state_json"
              exit 1
            fi
            sleep 10
          done
          echo "Smoke run timed out"
          exit 1

      - name: SQL smoke checks
        if: ${{ inputs.run_sql_smoke }}
        shell: bash
        run: |
          set -euo pipefail
          while IFS= read -r sql; do
            if [ -z "$sql" ]; then
              continue
            fi
            echo "Executing SQL smoke: $sql"
            stmt_json="$(jq -n --arg q "$sql" --arg wid "$SQL_WAREHOUSE_ID" '{statement: $q, warehouse_id: $wid, wait_timeout: "0s"}')"
            resp="$(databricks api post /api/2.0/sql/statements --json "$stmt_json")"
            stmt_id="$(echo "$resp" | jq -r '.statement_id')"
            if [ -z "$stmt_id" ] || [ "$stmt_id" = "null" ]; then
              echo "Failed to submit SQL statement"
              echo "$resp"
              exit 1
            fi
            statement_succeeded="false"
            for attempt in $(seq 1 60); do
              status_resp="$(databricks api get "/api/2.0/sql/statements/$stmt_id")"
              state="$(echo "$status_resp" | jq -r '.status.state')"
              echo "SQL statement $stmt_id state (attempt $attempt): $state"
              if [ "$state" = "SUCCEEDED" ]; then
                statement_succeeded="true"
                break
              fi
              if [ "$state" = "FAILED" ] || [ "$state" = "CANCELED" ] || [ "$state" = "CLOSED" ]; then
                echo "$status_resp"
                exit 1
              fi
              sleep 5
            done
            if [ "$statement_succeeded" != "true" ]; then
              echo "SQL smoke statement timed out: $stmt_id"
              exit 1
            fi
          done << 'EOF'
          ${{ inputs.sql_smoke_queries }}
          EOF

      - name: Asset validation notebook submit
        if: ${{ inputs.run_asset_validation }}
        shell: bash
        run: |
          set -euo pipefail
          run_output="$(databricks jobs submit --json "$(jq -n \
            --arg cid "$VALIDATION_CLUSTER_ID" \
            --arg path "${{ inputs.asset_validation_notebook_path }}" \
            --arg target "$TARGET" \
            --arg ref "${GITHUB_REF}" \
            --arg sha "${GITHUB_SHA}" \
            --arg run_id "${GITHUB_RUN_ID}" \
            '{
              run_name: "github-asset-validation-\($target)-\($run_id)",
              tasks: [
                {
                  task_key: "asset_validation",
                  existing_cluster_id: $cid,
                  notebook_task: {
                    notebook_path: $path,
                    base_parameters: {
                      target: $target,
                      git_ref: $ref,
                      commit_sha: $sha,
                      github_run_id: $run_id
                    }
                  }
                }
              ]
            }'
          )")"
          validation_run_id="$(echo "$run_output" | jq -r '.run_id')"
          if [ -z "$validation_run_id" ] || [ "$validation_run_id" = "null" ]; then
            echo "Failed to create asset validation run."
            echo "$run_output"
            exit 1
          fi
          echo "Submitted asset validation run_id=$validation_run_id"
          for attempt in $(seq 1 120); do
            state_json="$(databricks jobs get-run "$validation_run_id")"
            life_cycle_state="$(echo "$state_json" | jq -r '.state.life_cycle_state')"
            result_state="$(echo "$state_json" | jq -r '.state.result_state // empty')"
            echo "Asset validation state (attempt $attempt): $life_cycle_state result: ${result_state:-n/a}"
            if [ "$life_cycle_state" = "TERMINATED" ]; then
              if [ "$result_state" = "SUCCESS" ]; then
                exit 0
              fi
              echo "$state_json"
              exit 1
            fi
            if [ "$life_cycle_state" = "INTERNAL_ERROR" ] || [ "$life_cycle_state" = "SKIPPED" ]; then
              echo "$state_json"
              exit 1
            fi
            sleep 10
          done
          echo "Asset validation timed out"
          exit 1

      - name: Maturity metrics collection (bundle run)
        if: ${{ inputs.run_maturity_collect }}
        shell: bash
        run: |
          set -euo pipefail
          cd "$BUNDLE_PATH"
          extra_args=()
          if [ -n "${VALIDATION_CLUSTER_ID:-}" ]; then
            extra_args+=(--var "validation_cluster_id=$VALIDATION_CLUSTER_ID")
          fi
          databricks bundle run maturity_collect \
            --target "$TARGET" \
            "${extra_args[@]}" \
            --params env="$TARGET" \
            --params run_id="${GITHUB_RUN_ID}" \
            --params commit_sha="${GITHUB_SHA}" \
            --params repo="${GITHUB_REPOSITORY}" \
            --params git_ref="${GITHUB_REF}" \
            --params workflow_run_id="${GITHUB_RUN_ID}" \
            --params bundle_name="$BUNDLE_NAME_INPUT"

      - name: Maturity CI gate check (bundle run)
        if: ${{ inputs.run_maturity_ci_check }}
        shell: bash
        run: |
          set -euo pipefail
          cd "$BUNDLE_PATH"
          extra_args=()
          if [ -n "${VALIDATION_CLUSTER_ID:-}" ]; then
            extra_args+=(--var "validation_cluster_id=$VALIDATION_CLUSTER_ID")
          fi
          databricks bundle run maturity_ci_check \
            --target "$TARGET" \
            "${extra_args[@]}" \
            --params env="$TARGET" \
            --params enforcement_mode="$MATURITY_ENFORCEMENT_MODE_INPUT" \
            --params maturity_override="$MATURITY_OVERRIDE_INPUT"

      - name: Maturity scorecard status load (bundle run)
        if: ${{ inputs.run_maturity_scorecard_status_load }}
        shell: bash
        run: |
          set -euo pipefail
          cd "$BUNDLE_PATH"
          extra_args=()
          if [ -n "${VALIDATION_CLUSTER_ID:-}" ]; then
            extra_args+=(--var "validation_cluster_id=$VALIDATION_CLUSTER_ID")
          fi
          params=(--params env="$TARGET" --params updated_by="$MATURITY_SCORECARD_UPDATED_BY_INPUT")
          databricks bundle run "$MATURITY_SCORECARD_STATUS_JOB_KEY_INPUT" \
            --target "$TARGET" \
            "${extra_args[@]}" \
            "${params[@]}"

      - name: Maturity scorecard evaluation (bundle run)
        if: ${{ inputs.run_maturity_scorecard_eval }}
        shell: bash
        run: |
          set -euo pipefail
          cd "$BUNDLE_PATH"
          extra_args=()
          if [ -n "${VALIDATION_CLUSTER_ID:-}" ]; then
            extra_args+=(--var "validation_cluster_id=$VALIDATION_CLUSTER_ID")
          fi
          params=(--params env="$TARGET" --params run_id="${GITHUB_RUN_ID}" --params commit_sha="${GITHUB_SHA}")
          if [ -n "$MATURITY_SCORECARD_SOURCE_INPUT" ]; then
            params+=(--params scorecard_source="$MATURITY_SCORECARD_SOURCE_INPUT")
          fi
          if [ -n "$MATURITY_SCORECARD_PATH_INPUT" ]; then
            params+=(--params scorecard_path="$MATURITY_SCORECARD_PATH_INPUT")
          fi
          databricks bundle run "$MATURITY_SCORECARD_EVAL_JOB_KEY_INPUT" \
            --target "$TARGET" \
            "${extra_args[@]}" \
            "${params[@]}"

      - name: Maturity scorecard evidence stub (bundle run)
        if: ${{ inputs.run_maturity_scorecard_evidence_stub }}
        shell: bash
        run: |
          set -euo pipefail
          cd "$BUNDLE_PATH"
          extra_args=()
          if [ -n "${VALIDATION_CLUSTER_ID:-}" ]; then
            extra_args+=(--var "validation_cluster_id=$VALIDATION_CLUSTER_ID")
          fi
          params=(--params env="$TARGET" --params write_mode="$MATURITY_SCORECARD_EVIDENCE_STUB_WRITE_MODE_INPUT" --params updated_by="$MATURITY_SCORECARD_EVIDENCE_STUB_UPDATED_BY_INPUT")
          if [ -n "$MATURITY_SCORECARD_EVIDENCE_STUB_STATUS_PATH_INPUT" ]; then
            params+=(--params status_source_path="$MATURITY_SCORECARD_EVIDENCE_STUB_STATUS_PATH_INPUT")
          fi
          databricks bundle run "$MATURITY_SCORECARD_EVIDENCE_STUB_JOB_KEY_INPUT" \
            --target "$TARGET" \
            "${extra_args[@]}" \
            "${params[@]}"
